{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "questions = [\n",
    "    \"How to install nodejs\",\n",
    "    \"What are the benefits of meditation\",\n",
    "    \"How to make a chocolate cake\",\n",
    "    # Add more questions here\n",
    "]\n",
    "\n",
    "tags = ['js', 'health', 'cooking']\n",
    "\n",
    "# Preprocess the text using spaCy and remove stopwords using NLTK\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Preprocess all the questions\n",
    "questions = [preprocess_text(question) for question in questions]\n",
    "\n",
    "# Tokenize the text and convert to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions)\n",
    "sequences = tokenizer.texts_to_sequences(questions)\n",
    "\n",
    "# Pad sequences to ensure they have the same length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert tags to numerical labels\n",
    "tag_to_label = {tag: i for i, tag in enumerate(tags)}\n",
    "labels = [tag_to_label[tag] for tag in tags]\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(tags), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Add Early Stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with Early Stopping\n",
    "model.fit(sequences, labels, epochs=100, batch_size=1, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Now, you can use the model to predict the tag of a new question\n",
    "new_question = \"How to debug JavaScript code\"\n",
    "new_question = preprocess_text(new_question)\n",
    "new_sequence = tokenizer.texts_to_sequences([new_question])\n",
    "new_padded_sequence = pad_sequences(new_sequence, maxlen=max_sequence_length)\n",
    "predicted_label = model.predict_classes(new_padded_sequence)\n",
    "predicted_tag = tags[predicted_label[0]]\n",
    "\n",
    "print(\"Predicted tag:\", predicted_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
